# -*- coding: utf-8 -*-
"""DATA_Exploration_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gz4RI_xZ8CPXAi2AeIf3aNl3SBQ_dds5

# WEEK 2
"""

# Import necessary libraries for data analysis and visualization
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""### Step 1: Data Loading and Initial Exploration"""

# Load the dataset
import pandas as pd

filepaths = {
    "cough": r"D:\6190\MINOR IN AI PROJECT\datasets\cough.csv",
    "dengue": r"D:\6190\MINOR IN AI PROJECT\datasets\dengue.csv",
    "drugprescription": r"D:\6190\MINOR IN AI PROJECT\datasets\drugprescription.csv",
    "fever": r"D:\6190\MINOR IN AI PROJECT\datasets\fever.csv",
    "flu": r"D:\6190\MINOR IN AI PROJECT\datasets\flu.csv",
    "flu_cases": r"D:\6190\MINOR IN AI PROJECT\datasets\flu_cases_1997_2021_raw_data.csv",
    "ph_dengue": r"D:\6190\MINOR IN AI PROJECT\datasets\ph_dengue_cases2016-2020.csv",
    "pharmacy_sales": r"D:\6190\MINOR IN AI PROJECT\datasets\pharmacy_otc_sales_data.csv",
    "salesdaily": r"D:\6190\MINOR IN AI PROJECT\datasets\salesdaily.csv",
    "saleshourly": r"D:\6190\MINOR IN AI PROJECT\datasets\saleshourly.csv",
    "salesmonthly": r"D:\6190\MINOR IN AI PROJECT\datasets\salesmonthly.csv",
    "salesweekly": r"D:\6190\MINOR IN AI PROJECT\datasets\salesweekly.csv",
    "flu_meta": r"D:\6190\MINOR IN AI PROJECT\datasets\seasonal_VIW_FLU_METADATA.csv",
    "flu_fnt": r"D:\6190\MINOR IN AI PROJECT\datasets\seasonal_VIW_FNT.csv"
}

datasets = {}

for name, path in filepaths.items():
    datasets[name] = pd.read_csv(path)
    print(f"Loaded {name} → shape = {datasets[name].shape}")

# Get information about the columns, data types, and missing values
data = datasets['pharmacy_sales']
print(data.info())

# Get a summary of the numerical columns
print(data.describe())

# Get a summary of the categorical columns
data.describe(include = ['object'])

"""### Step 2: Detect Missing Values"""

# Check for missing values in the dataset
missing_values = data.isnull().sum()
print("Missing values in each column:\n", missing_values)

# Visualize missing values
(missing_values / data.shape[0]).sort_values().plot(kind='bar')
plt.title("Percentage of Missing Values by Column")
plt.show()

# Check for rows where 'PRODUCT_CATEGORY' is missing (Nan)
missing_values = data.isnull().sum()
print("Missing values in each column:\n", missing_values)
# Plot missing values
missing_values.plot(kind='bar', title='Missing Values by Column')
plt.show()

"""### Step 3: Identifying and Handling Duplicate Entries"""

# Check for duplicate entries across all columns
duplicate_entries = data[data.duplicated(keep=False)]

# Display the duplicate entries if any
if not duplicate_entries.empty:
    print(f"Number of duplicate entries found: {len(duplicate_entries)}")
    print(duplicate_entries.head(10))  # Display the first 10 duplicate entries for inspection
else:
    print("No duplicate entries found.")

# Remove duplicate entries (keeping the first occurrence)
data_cleaned = data.drop_duplicates(keep='first')
# Verify if duplicates are removed
print(f"Number of entries after removing duplicates: {data_cleaned.shape[0]}")

"""# Step 4: Correlation & Forecast"""

for name, df in datasets.items():
    # Normalize date column names
    if 'date' in df.columns:
        df.rename(columns={'date': 'Date'}, inplace=True)

    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df['year'] = df['Date'].dt.year
        df['month'] = df['Date'].dt.month
        df['week'] = df['Date'].dt.isocalendar().week
        df['day'] = df['Date'].dt.day

for name, df in datasets.items():
    print(name, " → columns:", df.columns.tolist())

# Fix pharmacy_sales column name
datasets['pharmacy_sales'].rename(columns={'Amount ($)': 'sales'}, inplace=True)
datasets['flu_cases'].rename(columns={'YEAR': 'year'}, inplace=True)
datasets['ph_dengue'].rename(columns={'Year': 'year'}, inplace=True)

print(datasets['pharmacy_sales'].columns)

# Sentiment Analysis using TextBlob
from textblob import TextBlob

datasets['cough']['sentiment'] = datasets['cough']['query'].apply(lambda x: TextBlob(x).sentiment.polarity)

keywords = ['cough', 'cold', 'flu', 'fever']

def label_text(txt):
    for k in keywords:
        if k in txt.lower():
            return k
    return 'other'

datasets['cough']['category'] = datasets['cough']['query'].apply(label_text)

# Normalize case columns
if 'cases' not in df.columns:
    if 'Dengue_Cases' in df.columns:
        df['cases'] = df['Dengue_Cases']
    if 'ILI' in df.columns:
        df['cases'] = df['ILI']

# Normalize sales columns
if 'sales' not in df.columns:
    if 'Amount ($)' in df.columns:
        df['sales'] = df['Amount ($)']

# Normalize the sales dataset
sales_df = datasets['salesmonthly'].copy()
sales_df['Date'] = pd.to_datetime(sales_df['datum'])
sales_df['year'] = sales_df['Date'].dt.year
sales_df.rename(columns={'R06':'sales'}, inplace=True)

# Combine datasets using overlapping years
combined = datasets['flu_cases'] \
    .merge(sales_df[['year', 'sales']], on='year', how='inner')

print(combined.head())

# Normalize YEAR column in all required datasets
datasets['flu_cases'].rename(columns={'YEAR': 'year'}, inplace=True)
datasets['ph_dengue'].rename(columns={'Year': 'year'}, inplace=True)
datasets['pharmacy_sales'].rename(columns={'Amount ($)': 'sales'}, inplace=True)

#visualizing
plt.figure(figsize=(10,5))
sns.lineplot(data=datasets['flu_cases'], x='year', y='Total_cases')
sns.lineplot(data=datasets['ph_dengue'], x='year', y='Dengue_Cases')
sns.lineplot(data=datasets['pharmacy_sales'], x='year', y='sales')

plt.legend()
plt.show()

datasets['cough'].groupby('query')['search interest'].sum().plot(kind='bar')
plt.title("Google Search Interest for Cough Keywords")
plt.show()

#Correlation & Pattern Analysis
merged = combined[['Total_cases', 'sales']].dropna()
print(merged.corr())

"""# Time series forecasting"""

# Time Series Forecasting using NeuralProphet
# ==============================
# Time Series Forecasting (NeuralProphet)
# ==============================
from neuralprophet import NeuralProphet
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import matplotlib.pyplot as plt

# ==========================
# Prepare dataset for Prophet
# ==========================
df = datasets['flu_cases'][['year', 'Total_cases']].copy()

df = df.groupby('year')['Total_cases'].sum().reset_index()
df.rename(columns={'year': 'ds', 'Total_cases': 'y'}, inplace=True)

# convert year to datetime safely
df['ds'] = pd.to_datetime(df['ds'], format='%Y')

# ensure only needed 2 columns
df = df[['ds', 'y']].copy()

# ==========================
# Build and train model
# ==========================
model = NeuralProphet(
    learning_rate=0.01,
    epochs=300,
    batch_size=16
)

# IMPORTANT: fit only once
model.fit(df, freq="YS")

# ==========================
# In-sample forecast
# ==========================
forecast = model.predict(df)

# detect prediction col automatically (yhat1 or yhat)
pred_col = 'yhat1' if 'yhat1' in forecast.columns else 'yhat'

actual = df['y'].values
pred = forecast[pred_col].values

rmse_np = np.sqrt(mean_squared_error(actual, pred))
mae_np = mean_absolute_error(actual, pred)

print("\n===== NeuralProphet Evaluation =====")
print("RMSE:", rmse_np)
print("MAE:", mae_np)

# ==========================
# Future Forecast
# ==========================
future = model.make_future_dataframe(df, periods=12)
forecast_future = model.predict(future)

# ==========================
# Plot
# ==========================
plt.figure(figsize=(10,5))
plt.plot(df['ds'], df['y'], label='Actual')
plt.plot(forecast['ds'], forecast[pred_col], label='Predicted')
plt.title("NeuralProphet Forecasting")
plt.legend()
plt.show()

# ======================
# BASELINE MODELS
# ======================

from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
import numpy as np

# Moving Average baseline
# Moving Average baseline
df_ma = df.copy()
df_ma['moving_avg'] = df_ma['y'].rolling(window=3).mean()

# drop rows where moving_avg is NaN so lengths match
baseline_df = df_ma.dropna(subset=['moving_avg'])

y_true = baseline_df['y'].values
y_pred = baseline_df['moving_avg'].values

mae_ma = mean_absolute_error(y_true, y_pred)
rmse_ma = np.sqrt(mean_squared_error(y_true, y_pred))

print("\n===== Moving Average Baseline =====")
print("MAE:", mae_ma)
print("RMSE:", rmse_ma)



"""# SUPERVISED LEARNING (Regression)"""

# -------------------------------
# Supervised Learning: Random Forest Regression
# -------------------------------

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Use overlapping combined dataset (flu vs sales)
ml_data = combined[['Total_cases', 'sales']].dropna()

X = ml_data[['Total_cases']]   # features
y = ml_data['sales']           # target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

preds = model.predict(X_test)

print("MAE:", mean_absolute_error(y_test, preds))
print("R2 Score:", r2_score(y_test, preds))


"""# Logistic Regression (Supervised Classification)"""

# ------------------------------------------
# Logistic Regression Classification
# ------------------------------------------

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Prepare data
cls = combined[['Total_cases', 'sales']].dropna()

threshold = cls['sales'].median()
cls['label'] = (cls['sales'] > threshold).astype(int)

X = cls[['Total_cases']]
y = cls['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

clf = LogisticRegression()
clf.fit(X_train, y_train)

preds = clf.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, preds))
print(classification_report(y_test, preds))

"""# CLUSTERING (K-Means)"""

# -------------------------------
# Clustering: K-Means
# -------------------------------

from sklearn.cluster import KMeans

cluster_data = combined[['Total_cases', 'sales']].dropna()

kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(cluster_data)

cluster_data['cluster'] = cluster_labels

print(cluster_data.head())

plt.scatter(cluster_data['Total_cases'], cluster_data['sales'], c=cluster_labels, cmap='viridis')
plt.xlabel('Flu Cases')
plt.ylabel('Sales')
plt.title('K-Means Clustering')
plt.show()

"""# DBSCAN Clustering"""

# ------------------------------------------
# Clustering Analysis: DBSCAN
# ------------------------------------------

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Select data
db_data = combined[['Total_cases', 'sales']].dropna()

# Scale data for DBSCAN
scaler = StandardScaler()
scaled = scaler.fit_transform(db_data)

# Run DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=3)
dbscan_labels = dbscan.fit_predict(scaled)

db_data['cluster'] = dbscan_labels

# Plot
plt.scatter(db_data['Total_cases'], db_data['sales'], c=dbscan_labels, cmap='plasma')
plt.xlabel('Total Flu Cases')
plt.ylabel('Medication Sales')
plt.title('DBSCAN Clustering')
plt.show()

print("DBSCAN clusters found:", set(dbscan_labels))

"""# ANOMALY DETECTION (Isolation Forest)"""

# -------------------------------
# Anomaly Detection: Isolation Forest
# -------------------------------

from sklearn.ensemble import IsolationForest

anom_data = combined[['Total_cases', 'sales']].dropna()

iso = IsolationForest(contamination=0.1, random_state=42)
anom_data['anomaly'] = iso.fit_predict(anom_data[['Total_cases', 'sales']])

print(anom_data.head())

# Plot anomalies
normal = anom_data[anom_data['anomaly'] == 1]
outliers = anom_data[anom_data['anomaly'] == -1]

plt.scatter(normal['Total_cases'], normal['sales'], c='blue', label='Normal')
plt.scatter(outliers['Total_cases'], outliers['sales'], c='red', label='Anomaly')
plt.xlabel('Flu Cases')
plt.ylabel('Sales')
plt.title('Isolation Forest Anomaly Detection')
plt.legend()
plt.show()

"""# WEEK 3

# Sentiment Transformer (BERT)
"""

# BERT Sentiment Analysis
from transformers import pipeline
import streamlit as st

# ================================
# BERT SENTIMENT TRANSFORMER ANALYSIS
# ================================

# ================================
# BERT SENTIMENT TRANSFORMER ANALYSIS
# ================================

from transformers import pipeline
import numpy as np
import matplotlib.pyplot as plt

# Assign sequential years
years = combined['year'].unique()
datasets['cough']['year'] = np.tile(
    years,
    int(np.ceil(len(datasets['cough']) / len(years)))
)[:len(datasets['cough'])]

# Load BERT
bert = pipeline("sentiment-analysis")

# Apply BERT
datasets['cough']['bert_sentiment'] = datasets['cough']['query'].apply(
    lambda x: bert(str(x))[0]['label']
)

# Convert labels
mapping = {'POSITIVE':1, 'NEGATIVE':-1, 'NEUTRAL':0}
datasets['cough']['bert_sentiment_score'] = datasets['cough']['bert_sentiment'].map(mapping)

# Aggregate by year  <-- PUT THIS BEFORE THE PLOT
sentiment_yearly = datasets['cough'].groupby('year')['bert_sentiment_score'].mean().reset_index()

# Plot
plt.figure(figsize=(10,5))
plt.plot(sentiment_yearly['year'], sentiment_yearly['bert_sentiment_score'], marker='o')
plt.title("Improved BERT Sentiment Trend (Cough Searches)")
plt.xlabel("Year")
plt.ylabel("Sentiment Score")
plt.grid(True)
plt.show()

# Merge into combined dataset
combined = combined.merge(sentiment_yearly, on='year', how='left')

# correlation check
print(combined[['Total_cases','sales','bert_sentiment_score']].corr())


"""SHAP EXPLAINABILITY"""
# ======================
# SHAP EXPLAINABILITY
# ======================
import shap

X = combined[['sales', 'bert_sentiment_score']].dropna()
y = combined['Total_cases'].dropna()

rf = RandomForestRegressor()
rf.fit(X, y)

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X)

shap.summary_plot(shap_values, X)

# ======================
# GOOGLE TRENDS DATA
# ======================
from pytrends.request import TrendReq

pytrends = TrendReq()
pytrends.build_payload(['flu'], timeframe='2015-01-01 2021-12-31')

trend = pytrends.interest_over_time().reset_index()
trend.rename(columns={'flu':'flu_trend'}, inplace=True)

# Merge into combined dataset
combined = combined.merge(trend, left_on='year', right_on=trend['date'].dt.year, how='left')
combined['flu_trend'] = combined['flu_trend'].fillna(0)

print(combined[['year','Total_cases','sales','flu_trend']].head())


"""# initial predictions / trends"""

#actual vs predicted
# actual vs predicted plot
plt.plot(df['ds'], df['y'], label='Actual')
plt.plot(forecast['ds'], forecast['yhat1'], label='Forecast')
plt.legend()
plt.show()

#HEAT MAPS
sns.heatmap(combined.corr(), annot=True)

#SEASONALITY PLOTS
from neuralprophet import NeuralProphet

# Re-initialize and fit the Prophet model as 'model_prophet' to avoid conflict
# Assuming df (from the original Prophet cell) is available in the kernel state
prophet_model = NeuralProphet()
prophet_model.fit(df)

# Now call plot_components on the correct Prophet model instance
prophet_model.plot_components(forecast)

"""# First Dashboard / Visualization Prototype"""

# Create final dashboard dataset
dashboard_df = combined.merge(sentiment_yearly, on='year', how='left')

combined.to_csv("combined.csv", index=False)
sentiment_yearly.to_csv("sentiment_yearly.csv", index=False)